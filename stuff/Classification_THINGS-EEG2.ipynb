{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminar operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from zipfile import ZipFile\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torcheeg import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using \"cuda\" device\n"
     ]
    }
   ],
   "source": [
    "# verify GPU availability\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using \\\"{}\\\" device\".format(device))\n",
    "# !nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**THINGS-EEG2 (subset: single subject)**\n",
    "\n",
    "RSVP: time-efficient rapid serial visual presentation\n",
    "- 1 subject\n",
    "- 17 channels\n",
    "- 1 second recording 100 samples (10ms or 100Hz freq)\n",
    "- 1854 concepts\n",
    "- 16740 images\n",
    "- 82160 trials\n",
    "- 27 or 53 high level categories (overlapped and missing categories)\n",
    "\n",
    "Train:\n",
    "- 1654 concepts\n",
    "- 16540 images (10 images per concept)\n",
    "- 66160 trial (4 rep per image)\n",
    "\n",
    "Test:\n",
    "- 200 concepts\n",
    "- 200 images (1 image per concept)\n",
    "- 16000 trial (80 rep per image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the path to the data\n",
    "data_path = '../data/THINGS-EEG2/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "eeg_train_path = os.path.join(data_path, 'preprocessed_data', 'sub-' + '04', 'preprocessed_eeg_' + 'training' + '.npy')\n",
    "eeg_test_path = os.path.join(data_path, 'preprocessed_data', 'sub-' + '04', 'preprocessed_eeg_' + 'test' + '.npy')\n",
    "image_metadata_path = os.path.join(data_path, 'image_metadata/image_metadata.npy')\n",
    "category53_path = os.path.join(data_path, 'image_metadata/category53_longFormat.tsv')\n",
    "category27_mat_path = os.path.join(data_path, 'image_metadata/category_mat_manual.tsv')\n",
    "\n",
    "eeg_train = np.load(eeg_train_path, allow_pickle=True).item()\n",
    "eeg_test = np.load(eeg_test_path, allow_pickle=True).item()\n",
    "image_metadata = np.load(image_metadata_path, allow_pickle=True).item()\n",
    "category53 = pd.read_csv(category53_path, sep='\\t')\n",
    "category27_mat = pd.read_csv(category27_mat_path, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10.474576271186441,\n",
       " 68.66666666666667,\n",
       " 54.529411764705884,\n",
       " 17.166666666666668,\n",
       " 48.78947368421053,\n",
       " 17.65714285714286,\n",
       " 50.108108108108105,\n",
       " 97.57894736842105,\n",
       " 25.054054054054053,\n",
       " 6.284745762711864,\n",
       " 54.529411764705884,\n",
       " 47.53846153846154,\n",
       " 41.2,\n",
       " 109.05882352941177,\n",
       " 92.7,\n",
       " 68.66666666666667,\n",
       " 68.66666666666667,\n",
       " 56.18181818181818,\n",
       " 74.16,\n",
       " 61.8,\n",
       " 39.4468085106383,\n",
       " 28.96875,\n",
       " 17.327102803738317,\n",
       " 54.529411764705884,\n",
       " 44.142857142857146,\n",
       " 26.485714285714284,\n",
       " 38.625]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(1854/np.sum(category27_mat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "animal                177\n",
       "bird                   27\n",
       "body part              34\n",
       "clothing              108\n",
       "clothing accessory     38\n",
       "container             105\n",
       "dessert                37\n",
       "drink                  19\n",
       "electronic device      74\n",
       "food                  295\n",
       "fruit                  34\n",
       "furniture              39\n",
       "home decor             45\n",
       "insect                 17\n",
       "kitchen appliance      20\n",
       "kitchen tool           27\n",
       "medical equipment      27\n",
       "musical instrument     33\n",
       "office supply          25\n",
       "part of car            30\n",
       "plant                  47\n",
       "sports equipment       64\n",
       "tool                  107\n",
       "toy                    34\n",
       "vegetable              42\n",
       "vehicle                70\n",
       "weapon                 48\n",
       "dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(category27_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9.474576271186441,\n",
       " 67.66666666666667,\n",
       " 53.529411764705884,\n",
       " 16.166666666666668,\n",
       " 47.78947368421053,\n",
       " 16.65714285714286,\n",
       " 49.108108108108105,\n",
       " 96.57894736842105,\n",
       " 24.054054054054053,\n",
       " 5.284745762711864,\n",
       " 53.529411764705884,\n",
       " 46.53846153846154,\n",
       " 40.2,\n",
       " 108.05882352941177,\n",
       " 91.7,\n",
       " 67.66666666666667,\n",
       " 67.66666666666667,\n",
       " 55.18181818181818,\n",
       " 73.16,\n",
       " 60.8,\n",
       " 38.4468085106383,\n",
       " 27.96875,\n",
       " 16.327102803738317,\n",
       " 53.529411764705884,\n",
       " 43.142857142857146,\n",
       " 25.485714285714284,\n",
       " 37.625]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(np.sum(category27_mat == 0) / np.sum(category27_mat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((16540, 4, 17, 100), (66160, 17, 100))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_images, n_repetitions, n_channels, n_times = eeg_train['preprocessed_eeg_data'].shape\n",
    "eeg_reshaped = eeg_train['preprocessed_eeg_data'].reshape((-1, n_channels, n_times))\n",
    "eeg_train['preprocessed_eeg_data'].shape, eeg_reshaped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.4539526 ,  0.51643129,  0.60930456,  0.34861743, -0.57875437,\n",
       "       -0.55656147, -0.80507541, -0.68757902, -0.21107484, -0.32593669,\n",
       "       -0.26585787,  0.40421187,  0.59717525,  0.6023611 , -0.12493843,\n",
       "       -1.20107749, -1.31159566, -0.34483615,  0.97501248,  1.24897328,\n",
       "        1.14696591,  0.46684753,  0.01703438,  0.73561328,  0.73216587,\n",
       "        0.26091281, -0.54657191, -1.16148978, -1.07541046, -0.13685725,\n",
       "        1.30884009,  1.54049018,  1.01553245,  0.16726818, -0.2563653 ,\n",
       "       -0.37234284,  0.12053667,  0.17917237,  0.68496869,  1.09932835,\n",
       "        0.94823464,  0.49821375, -0.29542823,  0.20053896,  0.59719814,\n",
       "        0.32353181,  0.20094806,  0.11625806,  0.04592397,  0.45675037,\n",
       "        0.88198026,  0.8293996 ,  0.25229832, -0.27092648, -0.33948166,\n",
       "       -0.1524662 , -0.28310139,  0.00259233,  0.55855998,  0.3931841 ,\n",
       "        0.5283516 ,  0.77513347,  0.72502785,  0.58410304,  0.56597055,\n",
       "       -0.23689939, -1.13033246, -0.95541466, -0.05806652,  0.60077673,\n",
       "        0.90789536,  1.19796229,  1.19308022,  1.43927699,  1.06027268,\n",
       "        0.74799612,  0.62691638, -0.05789964, -0.20696773, -0.18342142,\n",
       "       -0.27971977, -0.0411662 ,  0.08142963, -0.04702111,  0.01900339,\n",
       "        0.10553021, -0.46353158, -0.55539504, -0.51264698, -0.23162921,\n",
       "       -0.02225345,  0.70132951,  0.76322019,  0.31454537, -0.16575363,\n",
       "       -0.13470671,  0.03347468, -0.54944745, -0.99320759, -0.87425185])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eeg_reshaped[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((11, 5), (11,), (5,))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy import signal\n",
    "\n",
    "f, t, Sxx = signal.spectrogram(eeg_reshaped[0], nperseg=100, fs=100)\n",
    "f, t, Sxx = signal.spectrogram(eeg_reshaped[0][0], nperseg=20, fs=100)\n",
    "\n",
    "Sxx.shape, f.shape, t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.QuadMesh at 0x252ed537b50>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.pcolormesh(t, f, Sxx, shading='gouraud', figure=plt.figure(figsize=(8, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1. , 1.1],\n",
       "       [2. , 2.2],\n",
       "       [3. , 3.3],\n",
       "       [4. , 4.4]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([[[1, 1.1], [2, 2.2]], [[3, 3.3], [4, 4.4]]]).reshape((-1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_path = os.path.join(data_path, 'image_set_features', 'resnet18', 'training' + '_images.npy')\n",
    "features = np.load(features_path, allow_pickle=True)\n",
    "# features = features.reshape((-1, features.shape[-1]))\n",
    "\n",
    "# np.repeat(features, n_repetitions, axis=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1654, 10, 1, 512)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "block1.0.weight torch.Size([8, 1, 1, 64])\n",
      "block1.1.weight torch.Size([8])\n",
      "block1.1.bias torch.Size([8])\n",
      "block1.2.weight torch.Size([16, 1, 60, 1])\n",
      "block1.3.weight torch.Size([16])\n",
      "block1.3.bias torch.Size([16])\n",
      "block2.0.weight torch.Size([16, 1, 1, 16])\n",
      "block2.1.weight torch.Size([16, 16, 1, 1])\n",
      "block2.2.weight torch.Size([16])\n",
      "block2.2.bias torch.Size([16])\n",
      "lin.weight torch.Size([2, 64])\n"
     ]
    }
   ],
   "source": [
    "from Classification_EEG_models import EEGNet\n",
    "\n",
    "net = (EEGNet()).to('cuda')\n",
    "\n",
    "for i in net.named_parameters():\n",
    "    print(i[0], i[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 500)\n",
      "torch.Size([1, 3, 224, 224])\n",
      "(1, 2048)\n",
      "0.017908897\n",
      "[[0.00644116 0.         0.         ... 0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# from torchvision.models import resnet50, ResNet50_Weights\n",
    "# from PIL import Image\n",
    "# from torchvision.models.feature_extraction import create_feature_extractor\n",
    "\n",
    "# model = create_feature_extractor(resnet50(weights = ResNet50_Weights.DEFAULT), return_nodes={'flatten': 'flatten'}).to(device)\n",
    "# model.eval()\n",
    "\n",
    "# w = ResNet50_Weights.DEFAULT\n",
    "# preprocess = w.transforms()\n",
    "\n",
    "# part_dir = os.path.join(data_path, 'image_set', 'test_images.zip')\n",
    "# with ZipFile(part_dir, 'r') as zipObj:\n",
    "#     file = zipObj.open('test_images/00001_aircraft_carrier/aircraft_carrier_06s.jpg')\n",
    "#     img = Image.open(file).convert('RGB')\n",
    "#     input_img = preprocess(img).unsqueeze(0).to(device) # preprocess and convert to tensor\n",
    "\n",
    "#     print(img.size)\n",
    "#     input_img = preprocess(img).unsqueeze(0).to(device)\n",
    "#     print(input_img.shape)\n",
    "\n",
    "#     print(model(input_img)['flatten'].cpu().detach().numpy().shape)\n",
    "#     print(np.mean(model(input_img)['flatten'].cpu().detach().numpy()))\n",
    "#     print(model(input_img)['flatten'].cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['x', 'conv1', 'bn1', 'relu', 'maxpool', 'layer1.0.conv1', 'layer1.0.bn1', 'layer1.0.relu', 'layer1.0.conv2', 'layer1.0.bn2', 'layer1.0.add', 'layer1.0.relu_1', 'layer1.1.conv1', 'layer1.1.bn1', 'layer1.1.relu', 'layer1.1.conv2', 'layer1.1.bn2', 'layer1.1.add', 'layer1.1.relu_1', 'layer2.0.conv1', 'layer2.0.bn1', 'layer2.0.relu', 'layer2.0.conv2', 'layer2.0.bn2', 'layer2.0.downsample.0', 'layer2.0.downsample.1', 'layer2.0.add', 'layer2.0.relu_1', 'layer2.1.conv1', 'layer2.1.bn1', 'layer2.1.relu', 'layer2.1.conv2', 'layer2.1.bn2', 'layer2.1.add', 'layer2.1.relu_1', 'layer3.0.conv1', 'layer3.0.bn1', 'layer3.0.relu', 'layer3.0.conv2', 'layer3.0.bn2', 'layer3.0.downsample.0', 'layer3.0.downsample.1', 'layer3.0.add', 'layer3.0.relu_1', 'layer3.1.conv1', 'layer3.1.bn1', 'layer3.1.relu', 'layer3.1.conv2', 'layer3.1.bn2', 'layer3.1.add', 'layer3.1.relu_1', 'layer4.0.conv1', 'layer4.0.bn1', 'layer4.0.relu', 'layer4.0.conv2', 'layer4.0.bn2', 'layer4.0.downsample.0', 'layer4.0.downsample.1', 'layer4.0.add', 'layer4.0.relu_1', 'layer4.1.conv1', 'layer4.1.bn1', 'layer4.1.relu', 'layer4.1.conv2', 'layer4.1.bn2', 'layer4.1.add', 'layer4.1.relu_1', 'avgpool', 'flatten', 'fc'], ['x', 'conv1', 'bn1', 'relu', 'maxpool', 'layer1.0.conv1', 'layer1.0.bn1', 'layer1.0.relu', 'layer1.0.conv2', 'layer1.0.bn2', 'layer1.0.add', 'layer1.0.relu_1', 'layer1.1.conv1', 'layer1.1.bn1', 'layer1.1.relu', 'layer1.1.conv2', 'layer1.1.bn2', 'layer1.1.add', 'layer1.1.relu_1', 'layer2.0.conv1', 'layer2.0.bn1', 'layer2.0.relu', 'layer2.0.conv2', 'layer2.0.bn2', 'layer2.0.downsample.0', 'layer2.0.downsample.1', 'layer2.0.add', 'layer2.0.relu_1', 'layer2.1.conv1', 'layer2.1.bn1', 'layer2.1.relu', 'layer2.1.conv2', 'layer2.1.bn2', 'layer2.1.add', 'layer2.1.relu_1', 'layer3.0.conv1', 'layer3.0.bn1', 'layer3.0.relu', 'layer3.0.conv2', 'layer3.0.bn2', 'layer3.0.downsample.0', 'layer3.0.downsample.1', 'layer3.0.add', 'layer3.0.relu_1', 'layer3.1.conv1', 'layer3.1.bn1', 'layer3.1.relu', 'layer3.1.conv2', 'layer3.1.bn2', 'layer3.1.add', 'layer3.1.relu_1', 'layer4.0.conv1', 'layer4.0.bn1', 'layer4.0.relu', 'layer4.0.conv2', 'layer4.0.bn2', 'layer4.0.downsample.0', 'layer4.0.downsample.1', 'layer4.0.add', 'layer4.0.relu_1', 'layer4.1.conv1', 'layer4.1.bn1', 'layer4.1.relu', 'layer4.1.conv2', 'layer4.1.bn2', 'layer4.1.add', 'layer4.1.relu_1', 'avgpool', 'flatten', 'fc'])\n"
     ]
    }
   ],
   "source": [
    "from torchvision.models import efficientnet_b1, EfficientNet_B1_Weights, resnet18, ResNet18_Weights\n",
    "from torchvision.models.feature_extraction import create_feature_extractor, get_graph_node_names\n",
    "\n",
    "m = resnet18(weights = ResNet18_Weights.DEFAULT)\n",
    "print(get_graph_node_names(m))\n",
    "\n",
    "# model = create_feature_extractor(m, return_nodes={'avgpool': 'layer'}).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1654, 10, 1, 512)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tr_dir = os.path.join(data_path, 'image_set_features/resnet18/training_images.npy')\n",
    "# tr = np.load(a, allow_pickle=True)\n",
    "# tr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(np.mean(tr, axis=(0,1,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.6170788618500604"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from matplotlib import pyplot as plt\n",
    "\n",
    "# sum(np.reshape(tr, (-1)) == 0)/len(np.reshape(tr, (-1))) * 100\n",
    "# # sum(tr == 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['test_img_concepts', 'test_img_concepts_THINGS', 'test_img_files', 'train_img_files', 'train_img_concepts', 'train_img_concepts_THINGS'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_metadata.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('int64')"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category27_mat.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EEG signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16540, 4, 17, 100)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eeg_train['preprocessed_eeg_data'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_channels = 17\n",
    "# n_times = 100\n",
    "# n_classes = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(66160, 17, 100)\n",
      "(16000, 17, 100)\n"
     ]
    }
   ],
   "source": [
    "eeg_train_reshaped = eeg_train['preprocessed_eeg_data'].reshape((-1, 17, 100))\n",
    "eeg_test_reshaped = eeg_test['preprocessed_eeg_data'].reshape((-1, 17, 100))\n",
    "\n",
    "print(eeg_train_reshaped.shape)\n",
    "print(eeg_test_reshaped.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['test_img_concepts', 'test_img_concepts_THINGS', 'test_img_files', 'train_img_files', 'train_img_concepts', 'train_img_concepts_THINGS'])\n",
      "['00001_aardvark', '00001_aardvark', '00001_aardvark', '00001_aardvark', '00001_aardvark']\n",
      "['00001_aardvark', '00001_aardvark', '00001_aardvark', '00001_aardvark', '00001_aardvark']\n",
      "['aardvark_01b.jpg', 'aardvark_02s.jpg', 'aardvark_03s.jpg', 'aardvark_04s.jpg', 'aardvark_05s.jpg']\n"
     ]
    }
   ],
   "source": [
    "print(image_metadata.keys())\n",
    "print(image_metadata['train_img_concepts'][:5])\n",
    "print(image_metadata['train_img_concepts_THINGS'][:5])\n",
    "print(image_metadata['train_img_files'][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of files names in zip\n",
    "def get_concepts(partition):\n",
    "    path_tmp = os.path.join(data_path, 'image_set', f'{partition}_images.zip')\n",
    "\n",
    "    with ZipFile(path_tmp, 'r') as zipObj:\n",
    "        # Get list of files names in zip\n",
    "        files_list_tmp = zipObj.namelist()\n",
    "\n",
    "    files_list_tmp.sort()\n",
    "    concepts_tmp = ['_'.join(elem.split('/')[2].split('_')[: -1]) for elem in files_list_tmp if elem.endswith('/') == False]\n",
    "\n",
    "    concepts_tmp = np.repeat(concepts_tmp, 4 if partition == 'training' else 80)\n",
    "\n",
    "    return concepts_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66160\n",
      "16000\n"
     ]
    }
   ],
   "source": [
    "concepts_train = get_concepts('training')\n",
    "concepts_test = get_concepts('test')\n",
    "\n",
    "print(len(concepts_train))\n",
    "print(len(concepts_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concepts\n",
    "concepts_train_int = np.repeat(np.array(range(1654)), 40)\n",
    "concepts_test_int = np.repeat(np.array(range(200)), 80)\n",
    "\n",
    "concepts_test_dict = dict(zip(concepts_test_int, concepts_test)) # dictionary with concepts\n",
    "concepts_train_dict = dict(zip(concepts_train_int, concepts_train)) # dictionary with concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>concept</th>\n",
       "      <th>concept_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aardvark</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>abacus</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>accordion</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>acorn</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>air_conditioner</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           concept  concept_ID\n",
       "0         aardvark           0\n",
       "1           abacus           1\n",
       "2        accordion           2\n",
       "3            acorn           3\n",
       "4  air_conditioner           4"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# concept df\n",
    "concepts = sorted(list(set(concepts_train)) + list(set(concepts_test)))\n",
    "concepts_df = pd.DataFrame({'concept': concepts})\n",
    "concepts_df['concept_ID'] = concepts_df.index\n",
    "concepts_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>concept_ID</th>\n",
       "      <th>category27</th>\n",
       "      <th>category27_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[animal]</td>\n",
       "      <td>[0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[home decor]</td>\n",
       "      <td>[12]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[musical instrument]</td>\n",
       "      <td>[17]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>[electronic device]</td>\n",
       "      <td>[8]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>[furniture]</td>\n",
       "      <td>[11]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   concept_ID            category27 category27_ID\n",
       "0           0              [animal]           [0]\n",
       "1           1          [home decor]          [12]\n",
       "2           2  [musical instrument]          [17]\n",
       "3           4   [electronic device]           [8]\n",
       "4           5           [furniture]          [11]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# category27 df\n",
    "category27_df = category27_mat[category27_mat==1].stack().reset_index().drop(0, axis=1)\n",
    "category27_df.columns = ['concept_ID', 'category27']\n",
    "category27_df.sort_values(by=['category27'], inplace=True)\n",
    "category27_df['category27_ID'] = pd.factorize(category27_df['category27'])[0]\n",
    "category27_df = category27_df.groupby('concept_ID').agg({'category27': lambda x: list(x), 'category27_ID': lambda x: list(x)}).reset_index()\n",
    "category27_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>concept</th>\n",
       "      <th>category53</th>\n",
       "      <th>category53_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aardvark</td>\n",
       "      <td>[animal, mammal]</td>\n",
       "      <td>[0, 31]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>abacus</td>\n",
       "      <td>[home decor]</td>\n",
       "      <td>[25]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>accordion</td>\n",
       "      <td>[musical instrument]</td>\n",
       "      <td>[33]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>air_conditioner</td>\n",
       "      <td>[electronic device, home appliance]</td>\n",
       "      <td>[13, 24]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>air_mattress</td>\n",
       "      <td>[furniture]</td>\n",
       "      <td>[19]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           concept                           category53 category53_ID\n",
       "0         aardvark                     [animal, mammal]       [0, 31]\n",
       "1           abacus                         [home decor]          [25]\n",
       "2        accordion                 [musical instrument]          [33]\n",
       "3  air_conditioner  [electronic device, home appliance]      [13, 24]\n",
       "4     air_mattress                          [furniture]          [19]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# category53 df\n",
    "category53_df = category53.drop(columns='Word')\n",
    "category53_df.rename(columns={'category': 'category53', 'uniqueID': 'concept'}, inplace=True)\n",
    "category53_df.sort_values(by=['category53'], inplace=True)\n",
    "category53_df['category53_ID'] = pd.factorize(category53_df['category53'])[0]\n",
    "category53_df = category53_df.groupby('concept').agg({'category53': lambda x: list(x), 'category53_ID': lambda x: list(x)}).reset_index()\n",
    "category53_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>concept</th>\n",
       "      <th>concept_ID</th>\n",
       "      <th>category27</th>\n",
       "      <th>category27_ID</th>\n",
       "      <th>category53</th>\n",
       "      <th>category53_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aardvark</td>\n",
       "      <td>0</td>\n",
       "      <td>[animal]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[animal, mammal]</td>\n",
       "      <td>[0, 31]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>abacus</td>\n",
       "      <td>1</td>\n",
       "      <td>[home decor]</td>\n",
       "      <td>[12]</td>\n",
       "      <td>[home decor]</td>\n",
       "      <td>[25]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>accordion</td>\n",
       "      <td>2</td>\n",
       "      <td>[musical instrument]</td>\n",
       "      <td>[17]</td>\n",
       "      <td>[musical instrument]</td>\n",
       "      <td>[33]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>acorn</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>air_conditioner</td>\n",
       "      <td>4</td>\n",
       "      <td>[electronic device]</td>\n",
       "      <td>[8]</td>\n",
       "      <td>[electronic device, home appliance]</td>\n",
       "      <td>[13, 24]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1849</th>\n",
       "      <td>yoke</td>\n",
       "      <td>1849</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1850</th>\n",
       "      <td>yolk</td>\n",
       "      <td>1850</td>\n",
       "      <td>[food]</td>\n",
       "      <td>[9]</td>\n",
       "      <td>[food]</td>\n",
       "      <td>[16]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1851</th>\n",
       "      <td>zebra</td>\n",
       "      <td>1851</td>\n",
       "      <td>[animal]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[animal, mammal]</td>\n",
       "      <td>[0, 31]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1852</th>\n",
       "      <td>zipper</td>\n",
       "      <td>1852</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[fastener]</td>\n",
       "      <td>[15]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1853</th>\n",
       "      <td>zucchini</td>\n",
       "      <td>1853</td>\n",
       "      <td>[food, vegetable]</td>\n",
       "      <td>[9, 24]</td>\n",
       "      <td>[food, vegetable]</td>\n",
       "      <td>[16, 48]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1854 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              concept  concept_ID            category27 category27_ID  \\\n",
       "0            aardvark           0              [animal]           [0]   \n",
       "1              abacus           1          [home decor]          [12]   \n",
       "2           accordion           2  [musical instrument]          [17]   \n",
       "3               acorn           3                   NaN           NaN   \n",
       "4     air_conditioner           4   [electronic device]           [8]   \n",
       "...               ...         ...                   ...           ...   \n",
       "1849             yoke        1849                   NaN           NaN   \n",
       "1850             yolk        1850                [food]           [9]   \n",
       "1851            zebra        1851              [animal]           [0]   \n",
       "1852           zipper        1852                   NaN           NaN   \n",
       "1853         zucchini        1853     [food, vegetable]       [9, 24]   \n",
       "\n",
       "                               category53 category53_ID  \n",
       "0                        [animal, mammal]       [0, 31]  \n",
       "1                            [home decor]          [25]  \n",
       "2                    [musical instrument]          [33]  \n",
       "3                                     NaN           NaN  \n",
       "4     [electronic device, home appliance]      [13, 24]  \n",
       "...                                   ...           ...  \n",
       "1849                                  NaN           NaN  \n",
       "1850                               [food]          [16]  \n",
       "1851                     [animal, mammal]       [0, 31]  \n",
       "1852                           [fastener]          [15]  \n",
       "1853                    [food, vegetable]      [16, 48]  \n",
       "\n",
       "[1854 rows x 6 columns]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a dataframe with the concepts and their categories\n",
    "boh_df = pd.merge(left=concepts_df, right=category27_df, on='concept_ID', how='left')\n",
    "boh_df = pd.merge(left=boh_df, right=category53_df, on='concept', how='left')\n",
    "boh_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0    993\n",
       "NaN    559\n",
       "2.0    276\n",
       "3.0     26\n",
       "Name: category27, dtype: int64"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boh_df['category27'].str.len().value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0    784\n",
       "2.0    476\n",
       "NaN    406\n",
       "3.0    140\n",
       "4.0     42\n",
       "5.0      5\n",
       "6.0      1\n",
       "Name: category53, dtype: int64"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boh_df['category53'].str.len().value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EEGNET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test partition 200 classes\n",
    "- sub01 accuracy 20%\n",
    "- sub02 accuracy 15%\n",
    "- sub03 accuracy 25%\n",
    "- sub04 accuracy 20%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters and model\n",
    "\n",
    "model = models.EEGNet(chunk_size=100, num_electrodes=17, num_classes=200).to(device)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "epochs = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((12000, 17, 100), (4000, 17, 100))"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split\n",
    "len(eeg_test_reshaped)/200\n",
    "\n",
    "x_train_prova = []\n",
    "y_train_prova = []\n",
    "x_val_prova = []\n",
    "y_val_prova = []\n",
    "\n",
    "for i in range(0,80):\n",
    "    if i < 20:\n",
    "        x_val_prova.extend(eeg_test_reshaped[i::80])\n",
    "        y_val_prova.extend(concepts_test_int[i::80])\n",
    "    else:\n",
    "        x_train_prova.extend(eeg_test_reshaped[i::80])\n",
    "        y_train_prova.extend(concepts_test_int[i::80])\n",
    "\n",
    "x_train_prova = np.array(x_train_prova)\n",
    "x_val_prova = np.array(x_val_prova)\n",
    "y_train_prova = np.array(y_train_prova)\n",
    "y_val_prova = np.array(y_val_prova)\n",
    "\n",
    "x_train_prova.shape, x_val_prova.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data(Dataset):\n",
    "  def __init__(self, X_train, y_train):\n",
    "    self.X = torch.from_numpy(X_train.astype(np.float32)) # convert float64 to float32\n",
    "    self.y = torch.from_numpy(y_train).type(torch.LongTensor) # convert float64 to Long\n",
    "    self.len = self.X.shape[0]\n",
    "  \n",
    "  def __getitem__(self, index):\n",
    "    return self.X[index], self.y[index]\n",
    "  \n",
    "  def __len__(self):\n",
    "    return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = Data(x_train_prova, y_train_prova)\n",
    "data_val = Data(x_val_prova, y_val_prova)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 27,\n",
       " 28,\n",
       " 29,\n",
       " 30,\n",
       " 31,\n",
       " 32,\n",
       " 33,\n",
       " 34,\n",
       " 35,\n",
       " 36,\n",
       " 37,\n",
       " 38,\n",
       " 39,\n",
       " 40,\n",
       " 41,\n",
       " 42,\n",
       " 43,\n",
       " 44,\n",
       " 45,\n",
       " 46,\n",
       " 47,\n",
       " 48,\n",
       " 49,\n",
       " 50,\n",
       " 51,\n",
       " 52,\n",
       " 53,\n",
       " 54,\n",
       " 55,\n",
       " 56,\n",
       " 57,\n",
       " 58,\n",
       " 59,\n",
       " 60,\n",
       " 61,\n",
       " 62,\n",
       " 63,\n",
       " 64,\n",
       " 65,\n",
       " 66,\n",
       " 67,\n",
       " 68,\n",
       " 69,\n",
       " 70,\n",
       " 71,\n",
       " 72,\n",
       " 73,\n",
       " 74,\n",
       " 75,\n",
       " 76,\n",
       " 77,\n",
       " 78,\n",
       " 79,\n",
       " 80,\n",
       " 81,\n",
       " 82,\n",
       " 83,\n",
       " 84,\n",
       " 85,\n",
       " 86,\n",
       " 87,\n",
       " 88,\n",
       " 89,\n",
       " 90,\n",
       " 91,\n",
       " 92,\n",
       " 93,\n",
       " 94,\n",
       " 95,\n",
       " 96,\n",
       " 97,\n",
       " 98,\n",
       " 99,\n",
       " 100,\n",
       " 101,\n",
       " 102,\n",
       " 103,\n",
       " 104,\n",
       " 105,\n",
       " 106,\n",
       " 107,\n",
       " 108,\n",
       " 109,\n",
       " 110,\n",
       " 111,\n",
       " 112,\n",
       " 113,\n",
       " 114,\n",
       " 115,\n",
       " 116,\n",
       " 117,\n",
       " 118,\n",
       " 119,\n",
       " 120,\n",
       " 121,\n",
       " 122,\n",
       " 123,\n",
       " 124,\n",
       " 125,\n",
       " 126,\n",
       " 127,\n",
       " 128,\n",
       " 129,\n",
       " 130,\n",
       " 131,\n",
       " 132,\n",
       " 133,\n",
       " 134,\n",
       " 135,\n",
       " 136,\n",
       " 137,\n",
       " 138,\n",
       " 139,\n",
       " 140,\n",
       " 141,\n",
       " 142,\n",
       " 143,\n",
       " 144,\n",
       " 145,\n",
       " 146,\n",
       " 147,\n",
       " 148,\n",
       " 149,\n",
       " 150,\n",
       " 151,\n",
       " 152,\n",
       " 153,\n",
       " 154,\n",
       " 155,\n",
       " 156,\n",
       " 157,\n",
       " 158,\n",
       " 159,\n",
       " 160,\n",
       " 161,\n",
       " 162,\n",
       " 163,\n",
       " 164,\n",
       " 165,\n",
       " 166,\n",
       " 167,\n",
       " 168,\n",
       " 169,\n",
       " 170,\n",
       " 171,\n",
       " 172,\n",
       " 173,\n",
       " 174,\n",
       " 175,\n",
       " 176,\n",
       " 177,\n",
       " 178,\n",
       " 179,\n",
       " 180,\n",
       " 181,\n",
       " 182,\n",
       " 183,\n",
       " 184,\n",
       " 185,\n",
       " 186,\n",
       " 187,\n",
       " 188,\n",
       " 189,\n",
       " 190,\n",
       " 191,\n",
       " 192,\n",
       " 193,\n",
       " 194,\n",
       " 195,\n",
       " 196,\n",
       " 197,\n",
       " 198,\n",
       " 199,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 27,\n",
       " 28,\n",
       " 29,\n",
       " 30,\n",
       " 31,\n",
       " 32,\n",
       " 33,\n",
       " 34,\n",
       " 35,\n",
       " 36,\n",
       " 37,\n",
       " 38,\n",
       " 39,\n",
       " 40,\n",
       " 41,\n",
       " 42,\n",
       " 43,\n",
       " 44,\n",
       " 45,\n",
       " 46,\n",
       " 47,\n",
       " 48,\n",
       " 49,\n",
       " 50,\n",
       " 51,\n",
       " 52,\n",
       " 53,\n",
       " 54,\n",
       " 55,\n",
       " 56,\n",
       " 57,\n",
       " 58,\n",
       " 59,\n",
       " 60,\n",
       " 61,\n",
       " 62,\n",
       " 63,\n",
       " 64,\n",
       " 65,\n",
       " 66,\n",
       " 67,\n",
       " 68,\n",
       " 69,\n",
       " 70,\n",
       " 71,\n",
       " 72,\n",
       " 73,\n",
       " 74,\n",
       " 75,\n",
       " 76,\n",
       " 77,\n",
       " 78,\n",
       " 79,\n",
       " 80,\n",
       " 81,\n",
       " 82,\n",
       " 83,\n",
       " 84,\n",
       " 85,\n",
       " 86,\n",
       " 87,\n",
       " 88,\n",
       " 89,\n",
       " 90,\n",
       " 91,\n",
       " 92,\n",
       " 93,\n",
       " 94,\n",
       " 95,\n",
       " 96,\n",
       " 97,\n",
       " 98,\n",
       " 99,\n",
       " 100,\n",
       " 101,\n",
       " 102,\n",
       " 103,\n",
       " 104,\n",
       " 105,\n",
       " 106,\n",
       " 107,\n",
       " 108,\n",
       " 109,\n",
       " 110,\n",
       " 111,\n",
       " 112,\n",
       " 113,\n",
       " 114,\n",
       " 115,\n",
       " 116,\n",
       " 117,\n",
       " 118,\n",
       " 119,\n",
       " 120,\n",
       " 121,\n",
       " 122,\n",
       " 123,\n",
       " 124,\n",
       " 125,\n",
       " 126,\n",
       " 127,\n",
       " 128,\n",
       " 129,\n",
       " 130,\n",
       " 131,\n",
       " 132,\n",
       " 133,\n",
       " 134,\n",
       " 135,\n",
       " 136,\n",
       " 137,\n",
       " 138,\n",
       " 139,\n",
       " 140,\n",
       " 141,\n",
       " 142,\n",
       " 143,\n",
       " 144,\n",
       " 145,\n",
       " 146,\n",
       " 147,\n",
       " 148,\n",
       " 149,\n",
       " 150,\n",
       " 151,\n",
       " 152,\n",
       " 153,\n",
       " 154,\n",
       " 155,\n",
       " 156,\n",
       " 157,\n",
       " 158,\n",
       " 159,\n",
       " 160,\n",
       " 161,\n",
       " 162,\n",
       " 163,\n",
       " 164,\n",
       " 165,\n",
       " 166,\n",
       " 167,\n",
       " 168,\n",
       " 169,\n",
       " 170,\n",
       " 171,\n",
       " 172,\n",
       " 173,\n",
       " 174,\n",
       " 175,\n",
       " 176,\n",
       " 177,\n",
       " 178,\n",
       " 179,\n",
       " 180,\n",
       " 181,\n",
       " 182,\n",
       " 183,\n",
       " 184,\n",
       " 185,\n",
       " 186,\n",
       " 187,\n",
       " 188,\n",
       " 189,\n",
       " 190,\n",
       " 191,\n",
       " 192,\n",
       " 193,\n",
       " 194,\n",
       " 195,\n",
       " 196,\n",
       " 197,\n",
       " 198,\n",
       " 199,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 27,\n",
       " 28,\n",
       " 29,\n",
       " 30,\n",
       " 31,\n",
       " 32,\n",
       " 33,\n",
       " 34,\n",
       " 35,\n",
       " 36,\n",
       " 37,\n",
       " 38,\n",
       " 39,\n",
       " 40,\n",
       " 41,\n",
       " 42,\n",
       " 43,\n",
       " 44,\n",
       " 45,\n",
       " 46,\n",
       " 47,\n",
       " 48,\n",
       " 49,\n",
       " 50,\n",
       " 51,\n",
       " 52,\n",
       " 53,\n",
       " 54,\n",
       " 55,\n",
       " 56,\n",
       " 57,\n",
       " 58,\n",
       " 59,\n",
       " 60,\n",
       " 61,\n",
       " 62,\n",
       " 63,\n",
       " 64,\n",
       " 65,\n",
       " 66,\n",
       " 67,\n",
       " 68,\n",
       " 69,\n",
       " 70,\n",
       " 71,\n",
       " 72,\n",
       " 73,\n",
       " 74,\n",
       " 75,\n",
       " 76,\n",
       " 77,\n",
       " 78,\n",
       " 79,\n",
       " 80,\n",
       " 81,\n",
       " 82,\n",
       " 83,\n",
       " 84,\n",
       " 85,\n",
       " 86,\n",
       " 87,\n",
       " 88,\n",
       " 89,\n",
       " 90,\n",
       " 91,\n",
       " 92,\n",
       " 93,\n",
       " 94,\n",
       " 95,\n",
       " 96,\n",
       " 97,\n",
       " 98,\n",
       " 99,\n",
       " 100,\n",
       " 101,\n",
       " 102,\n",
       " 103,\n",
       " 104,\n",
       " 105,\n",
       " 106,\n",
       " 107,\n",
       " 108,\n",
       " 109,\n",
       " 110,\n",
       " 111,\n",
       " 112,\n",
       " 113,\n",
       " 114,\n",
       " 115,\n",
       " 116,\n",
       " 117,\n",
       " 118,\n",
       " 119,\n",
       " 120,\n",
       " 121,\n",
       " 122,\n",
       " 123,\n",
       " 124,\n",
       " 125,\n",
       " 126,\n",
       " 127,\n",
       " 128,\n",
       " 129,\n",
       " 130,\n",
       " 131,\n",
       " 132,\n",
       " 133,\n",
       " 134,\n",
       " 135,\n",
       " 136,\n",
       " 137,\n",
       " 138,\n",
       " 139,\n",
       " 140,\n",
       " 141,\n",
       " 142,\n",
       " 143,\n",
       " 144,\n",
       " 145,\n",
       " 146,\n",
       " 147,\n",
       " 148,\n",
       " 149,\n",
       " 150,\n",
       " 151,\n",
       " 152,\n",
       " 153,\n",
       " 154,\n",
       " 155,\n",
       " 156,\n",
       " 157,\n",
       " 158,\n",
       " 159,\n",
       " 160,\n",
       " 161,\n",
       " 162,\n",
       " 163,\n",
       " 164,\n",
       " 165,\n",
       " 166,\n",
       " 167,\n",
       " 168,\n",
       " 169,\n",
       " 170,\n",
       " 171,\n",
       " 172,\n",
       " 173,\n",
       " 174,\n",
       " 175,\n",
       " 176,\n",
       " 177,\n",
       " 178,\n",
       " 179,\n",
       " 180,\n",
       " 181,\n",
       " 182,\n",
       " 183,\n",
       " 184,\n",
       " 185,\n",
       " 186,\n",
       " 187,\n",
       " 188,\n",
       " 189,\n",
       " 190,\n",
       " 191,\n",
       " 192,\n",
       " 193,\n",
       " 194,\n",
       " 195,\n",
       " 196,\n",
       " 197,\n",
       " 198,\n",
       " 199,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 27,\n",
       " 28,\n",
       " 29,\n",
       " 30,\n",
       " 31,\n",
       " 32,\n",
       " 33,\n",
       " 34,\n",
       " 35,\n",
       " 36,\n",
       " 37,\n",
       " 38,\n",
       " 39,\n",
       " 40,\n",
       " 41,\n",
       " 42,\n",
       " 43,\n",
       " 44,\n",
       " 45,\n",
       " 46,\n",
       " 47,\n",
       " 48,\n",
       " 49,\n",
       " 50,\n",
       " 51,\n",
       " 52,\n",
       " 53,\n",
       " 54,\n",
       " 55,\n",
       " 56,\n",
       " 57,\n",
       " 58,\n",
       " 59,\n",
       " 60,\n",
       " 61,\n",
       " 62,\n",
       " 63,\n",
       " 64,\n",
       " 65,\n",
       " 66,\n",
       " 67,\n",
       " 68,\n",
       " 69,\n",
       " 70,\n",
       " 71,\n",
       " 72,\n",
       " 73,\n",
       " 74,\n",
       " 75,\n",
       " 76,\n",
       " 77,\n",
       " 78,\n",
       " 79,\n",
       " 80,\n",
       " 81,\n",
       " 82,\n",
       " 83,\n",
       " 84,\n",
       " 85,\n",
       " 86,\n",
       " 87,\n",
       " 88,\n",
       " 89,\n",
       " 90,\n",
       " 91,\n",
       " 92,\n",
       " 93,\n",
       " 94,\n",
       " 95,\n",
       " 96,\n",
       " 97,\n",
       " 98,\n",
       " 99,\n",
       " 100,\n",
       " 101,\n",
       " 102,\n",
       " 103,\n",
       " 104,\n",
       " 105,\n",
       " 106,\n",
       " 107,\n",
       " 108,\n",
       " 109,\n",
       " 110,\n",
       " 111,\n",
       " 112,\n",
       " 113,\n",
       " 114,\n",
       " 115,\n",
       " 116,\n",
       " 117,\n",
       " 118,\n",
       " 119,\n",
       " 120,\n",
       " 121,\n",
       " 122,\n",
       " 123,\n",
       " 124,\n",
       " 125,\n",
       " 126,\n",
       " 127,\n",
       " 128,\n",
       " 129,\n",
       " 130,\n",
       " 131,\n",
       " 132,\n",
       " 133,\n",
       " 134,\n",
       " 135,\n",
       " 136,\n",
       " 137,\n",
       " 138,\n",
       " 139,\n",
       " 140,\n",
       " 141,\n",
       " 142,\n",
       " 143,\n",
       " 144,\n",
       " 145,\n",
       " 146,\n",
       " 147,\n",
       " 148,\n",
       " 149,\n",
       " 150,\n",
       " 151,\n",
       " 152,\n",
       " 153,\n",
       " 154,\n",
       " 155,\n",
       " 156,\n",
       " 157,\n",
       " 158,\n",
       " 159,\n",
       " 160,\n",
       " 161,\n",
       " 162,\n",
       " 163,\n",
       " 164,\n",
       " 165,\n",
       " 166,\n",
       " 167,\n",
       " 168,\n",
       " 169,\n",
       " 170,\n",
       " 171,\n",
       " 172,\n",
       " 173,\n",
       " 174,\n",
       " 175,\n",
       " 176,\n",
       " 177,\n",
       " 178,\n",
       " 179,\n",
       " 180,\n",
       " 181,\n",
       " 182,\n",
       " 183,\n",
       " 184,\n",
       " 185,\n",
       " 186,\n",
       " 187,\n",
       " 188,\n",
       " 189,\n",
       " 190,\n",
       " 191,\n",
       " 192,\n",
       " 193,\n",
       " 194,\n",
       " 195,\n",
       " 196,\n",
       " 197,\n",
       " 198,\n",
       " 199,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 27,\n",
       " 28,\n",
       " 29,\n",
       " 30,\n",
       " 31,\n",
       " 32,\n",
       " 33,\n",
       " 34,\n",
       " 35,\n",
       " 36,\n",
       " 37,\n",
       " 38,\n",
       " 39,\n",
       " 40,\n",
       " 41,\n",
       " 42,\n",
       " 43,\n",
       " 44,\n",
       " 45,\n",
       " 46,\n",
       " 47,\n",
       " 48,\n",
       " 49,\n",
       " 50,\n",
       " 51,\n",
       " 52,\n",
       " 53,\n",
       " 54,\n",
       " 55,\n",
       " 56,\n",
       " 57,\n",
       " 58,\n",
       " 59,\n",
       " 60,\n",
       " 61,\n",
       " 62,\n",
       " 63,\n",
       " 64,\n",
       " 65,\n",
       " 66,\n",
       " 67,\n",
       " 68,\n",
       " 69,\n",
       " 70,\n",
       " 71,\n",
       " 72,\n",
       " 73,\n",
       " 74,\n",
       " 75,\n",
       " 76,\n",
       " 77,\n",
       " 78,\n",
       " 79,\n",
       " 80,\n",
       " 81,\n",
       " 82,\n",
       " 83,\n",
       " 84,\n",
       " 85,\n",
       " 86,\n",
       " 87,\n",
       " 88,\n",
       " 89,\n",
       " 90,\n",
       " 91,\n",
       " 92,\n",
       " 93,\n",
       " 94,\n",
       " 95,\n",
       " 96,\n",
       " 97,\n",
       " 98,\n",
       " 99,\n",
       " 100,\n",
       " 101,\n",
       " 102,\n",
       " 103,\n",
       " 104,\n",
       " 105,\n",
       " 106,\n",
       " 107,\n",
       " 108,\n",
       " 109,\n",
       " 110,\n",
       " 111,\n",
       " 112,\n",
       " 113,\n",
       " 114,\n",
       " 115,\n",
       " 116,\n",
       " 117,\n",
       " 118,\n",
       " 119,\n",
       " 120,\n",
       " 121,\n",
       " 122,\n",
       " 123,\n",
       " 124,\n",
       " 125,\n",
       " 126,\n",
       " 127,\n",
       " 128,\n",
       " 129,\n",
       " 130,\n",
       " 131,\n",
       " 132,\n",
       " 133,\n",
       " 134,\n",
       " 135,\n",
       " 136,\n",
       " 137,\n",
       " 138,\n",
       " 139,\n",
       " 140,\n",
       " 141,\n",
       " 142,\n",
       " 143,\n",
       " 144,\n",
       " 145,\n",
       " 146,\n",
       " 147,\n",
       " 148,\n",
       " 149,\n",
       " 150,\n",
       " 151,\n",
       " 152,\n",
       " 153,\n",
       " 154,\n",
       " 155,\n",
       " 156,\n",
       " 157,\n",
       " 158,\n",
       " 159,\n",
       " 160,\n",
       " 161,\n",
       " 162,\n",
       " 163,\n",
       " 164,\n",
       " 165,\n",
       " 166,\n",
       " 167,\n",
       " 168,\n",
       " 169,\n",
       " 170,\n",
       " 171,\n",
       " 172,\n",
       " 173,\n",
       " 174,\n",
       " 175,\n",
       " 176,\n",
       " 177,\n",
       " 178,\n",
       " 179,\n",
       " 180,\n",
       " 181,\n",
       " 182,\n",
       " 183,\n",
       " 184,\n",
       " 185,\n",
       " 186,\n",
       " 187,\n",
       " 188,\n",
       " 189,\n",
       " 190,\n",
       " 191,\n",
       " 192,\n",
       " 193,\n",
       " 194,\n",
       " 195,\n",
       " 196,\n",
       " 197,\n",
       " 198,\n",
       " 199,\n",
       " ...]"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# next(iter(data_train))\n",
    "# iter(data_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'batch_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[124], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m data_train \u001b[39m=\u001b[39m Data(x_train_prova, y_train_prova)\n\u001b[0;32m      3\u001b[0m data_val \u001b[39m=\u001b[39m Data(x_val_prova, y_val_prova)\n\u001b[1;32m----> 5\u001b[0m train_loader \u001b[39m=\u001b[39m DataLoader(data_train, batch_size\u001b[39m=\u001b[39mbatch_size, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m      6\u001b[0m val_loader \u001b[39m=\u001b[39m DataLoader(data_val, batch_size\u001b[39m=\u001b[39mbatch_size, shuffle\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'batch_size' is not defined"
     ]
    }
   ],
   "source": [
    "# dataset and dataloader\n",
    "data_train = Data(x_train_prova, y_train_prova)\n",
    "data_val = Data(x_val_prova, y_val_prova)\n",
    "\n",
    "train_loader = DataLoader(data_train, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(data_val, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch_idx, batch in enumerate(dataloader):\n",
    "        X = batch[0].to(device)\n",
    "        y = batch[1].to(device)\n",
    "\n",
    "        X = X.reshape(batch_size, 1, 17, 100)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            loss, current = loss.item(), batch_idx * len(X)\n",
    "            # print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def valid(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    val_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            X = batch[0].to(device)\n",
    "            y = batch[1].to(device)\n",
    "\n",
    "            X = X.reshape(batch_size, 1, 17, 100)\n",
    "\n",
    "            pred = model(X)\n",
    "            val_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    val_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Val accuracy: {(100*correct):>0.1f}%, Val avg loss: {val_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ Epoch 1 ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val accuracy: 2.8%, Val avg loss: 4.905233 \n",
      "\n",
      "------------ Epoch 2 ---------------\n",
      "Val accuracy: 5.9%, Val avg loss: 4.605338 \n",
      "\n",
      "------------ Epoch 3 ---------------\n",
      "Val accuracy: 6.7%, Val avg loss: 4.453997 \n",
      "\n",
      "------------ Epoch 4 ---------------\n",
      "Val accuracy: 9.4%, Val avg loss: 4.339848 \n",
      "\n",
      "------------ Epoch 5 ---------------\n",
      "Val accuracy: 10.0%, Val avg loss: 4.251158 \n",
      "\n",
      "------------ Epoch 6 ---------------\n",
      "Val accuracy: 11.5%, Val avg loss: 4.189923 \n",
      "\n",
      "------------ Epoch 7 ---------------\n",
      "Val accuracy: 12.4%, Val avg loss: 4.144713 \n",
      "\n",
      "------------ Epoch 8 ---------------\n",
      "Val accuracy: 12.2%, Val avg loss: 4.081394 \n",
      "\n",
      "------------ Epoch 9 ---------------\n",
      "Val accuracy: 13.1%, Val avg loss: 4.056725 \n",
      "\n",
      "------------ Epoch 10 ---------------\n",
      "Val accuracy: 12.7%, Val avg loss: 4.042118 \n",
      "\n",
      "------------ Epoch 11 ---------------\n",
      "Val accuracy: 13.6%, Val avg loss: 4.022032 \n",
      "\n",
      "------------ Epoch 12 ---------------\n",
      "Val accuracy: 14.1%, Val avg loss: 4.012230 \n",
      "\n",
      "------------ Epoch 13 ---------------\n",
      "Val accuracy: 14.3%, Val avg loss: 3.972534 \n",
      "\n",
      "------------ Epoch 14 ---------------\n",
      "Val accuracy: 14.2%, Val avg loss: 3.969154 \n",
      "\n",
      "------------ Epoch 15 ---------------\n",
      "Val accuracy: 13.4%, Val avg loss: 3.964015 \n",
      "\n",
      "------------ Epoch 16 ---------------\n",
      "Val accuracy: 14.4%, Val avg loss: 3.926499 \n",
      "\n",
      "------------ Epoch 17 ---------------\n",
      "Val accuracy: 14.9%, Val avg loss: 3.927060 \n",
      "\n",
      "------------ Epoch 18 ---------------\n",
      "Val accuracy: 14.6%, Val avg loss: 3.932419 \n",
      "\n",
      "------------ Epoch 19 ---------------\n",
      "Val accuracy: 15.0%, Val avg loss: 3.913558 \n",
      "\n",
      "------------ Epoch 20 ---------------\n",
      "Val accuracy: 14.6%, Val avg loss: 3.904003 \n",
      "\n",
      "------------ Epoch 21 ---------------\n",
      "Val accuracy: 15.4%, Val avg loss: 3.890239 \n",
      "\n",
      "------------ Epoch 22 ---------------\n",
      "Val accuracy: 16.1%, Val avg loss: 3.864564 \n",
      "\n",
      "------------ Epoch 23 ---------------\n",
      "Val accuracy: 15.2%, Val avg loss: 3.869253 \n",
      "\n",
      "------------ Epoch 24 ---------------\n",
      "Val accuracy: 16.4%, Val avg loss: 3.830568 \n",
      "\n",
      "------------ Epoch 25 ---------------\n",
      "Val accuracy: 15.5%, Val avg loss: 3.854290 \n",
      "\n",
      "------------ Epoch 26 ---------------\n",
      "Val accuracy: 15.9%, Val avg loss: 3.863745 \n",
      "\n",
      "------------ Epoch 27 ---------------\n",
      "Val accuracy: 15.7%, Val avg loss: 3.854878 \n",
      "\n",
      "------------ Epoch 28 ---------------\n",
      "Val accuracy: 16.4%, Val avg loss: 3.833096 \n",
      "\n",
      "------------ Epoch 29 ---------------\n",
      "Val accuracy: 16.8%, Val avg loss: 3.836442 \n",
      "\n",
      "------------ Epoch 30 ---------------\n",
      "Val accuracy: 16.3%, Val avg loss: 3.813673 \n",
      "\n",
      "------------ Epoch 31 ---------------\n",
      "Val accuracy: 17.0%, Val avg loss: 3.788044 \n",
      "\n",
      "------------ Epoch 32 ---------------\n",
      "Val accuracy: 17.5%, Val avg loss: 3.817786 \n",
      "\n",
      "------------ Epoch 33 ---------------\n",
      "Val accuracy: 17.8%, Val avg loss: 3.790020 \n",
      "\n",
      "------------ Epoch 34 ---------------\n",
      "Val accuracy: 17.1%, Val avg loss: 3.815639 \n",
      "\n",
      "------------ Epoch 35 ---------------\n",
      "Val accuracy: 18.5%, Val avg loss: 3.763598 \n",
      "\n",
      "------------ Epoch 36 ---------------\n",
      "Val accuracy: 18.5%, Val avg loss: 3.774070 \n",
      "\n",
      "------------ Epoch 37 ---------------\n",
      "Val accuracy: 18.4%, Val avg loss: 3.795791 \n",
      "\n",
      "------------ Epoch 38 ---------------\n",
      "Val accuracy: 17.7%, Val avg loss: 3.784087 \n",
      "\n",
      "------------ Epoch 39 ---------------\n",
      "Val accuracy: 16.1%, Val avg loss: 3.841228 \n",
      "\n",
      "------------ Epoch 40 ---------------\n",
      "Val accuracy: 18.4%, Val avg loss: 3.768401 \n",
      "\n",
      "------------ Epoch 41 ---------------\n",
      "Val accuracy: 18.3%, Val avg loss: 3.747480 \n",
      "\n",
      "------------ Epoch 42 ---------------\n",
      "Val accuracy: 17.5%, Val avg loss: 3.765612 \n",
      "\n",
      "------------ Epoch 43 ---------------\n",
      "Val accuracy: 17.5%, Val avg loss: 3.755127 \n",
      "\n",
      "------------ Epoch 44 ---------------\n",
      "Val accuracy: 18.6%, Val avg loss: 3.756942 \n",
      "\n",
      "------------ Epoch 45 ---------------\n",
      "Val accuracy: 18.4%, Val avg loss: 3.752184 \n",
      "\n",
      "------------ Epoch 46 ---------------\n",
      "Val accuracy: 18.2%, Val avg loss: 3.744883 \n",
      "\n",
      "------------ Epoch 47 ---------------\n",
      "Val accuracy: 17.7%, Val avg loss: 3.749209 \n",
      "\n",
      "------------ Epoch 48 ---------------\n",
      "Val accuracy: 19.4%, Val avg loss: 3.722234 \n",
      "\n",
      "------------ Epoch 49 ---------------\n",
      "Val accuracy: 19.6%, Val avg loss: 3.720726 \n",
      "\n",
      "------------ Epoch 50 ---------------\n",
      "Val accuracy: 18.3%, Val avg loss: 3.721468 \n",
      "\n",
      "------------ Epoch 51 ---------------\n",
      "Val accuracy: 18.6%, Val avg loss: 3.737296 \n",
      "\n",
      "------------ Epoch 52 ---------------\n",
      "Val accuracy: 18.6%, Val avg loss: 3.726822 \n",
      "\n",
      "------------ Epoch 53 ---------------\n",
      "Val accuracy: 18.6%, Val avg loss: 3.755808 \n",
      "\n",
      "------------ Epoch 54 ---------------\n",
      "Val accuracy: 19.3%, Val avg loss: 3.702790 \n",
      "\n",
      "------------ Epoch 55 ---------------\n",
      "Val accuracy: 17.0%, Val avg loss: 3.760911 \n",
      "\n",
      "------------ Epoch 56 ---------------\n",
      "Val accuracy: 19.0%, Val avg loss: 3.695403 \n",
      "\n",
      "------------ Epoch 57 ---------------\n",
      "Val accuracy: 19.2%, Val avg loss: 3.696495 \n",
      "\n",
      "------------ Epoch 58 ---------------\n",
      "Val accuracy: 19.2%, Val avg loss: 3.682638 \n",
      "\n",
      "------------ Epoch 59 ---------------\n",
      "Val accuracy: 18.9%, Val avg loss: 3.720255 \n",
      "\n",
      "------------ Epoch 60 ---------------\n",
      "Val accuracy: 18.3%, Val avg loss: 3.750596 \n",
      "\n",
      "------------ Epoch 61 ---------------\n",
      "Val accuracy: 19.7%, Val avg loss: 3.707602 \n",
      "\n",
      "------------ Epoch 62 ---------------\n",
      "Val accuracy: 19.7%, Val avg loss: 3.697912 \n",
      "\n",
      "------------ Epoch 63 ---------------\n",
      "Val accuracy: 18.1%, Val avg loss: 3.706244 \n",
      "\n",
      "------------ Epoch 64 ---------------\n",
      "Val accuracy: 19.9%, Val avg loss: 3.675769 \n",
      "\n",
      "------------ Epoch 65 ---------------\n",
      "Val accuracy: 20.5%, Val avg loss: 3.675567 \n",
      "\n",
      "------------ Epoch 66 ---------------\n",
      "Val accuracy: 19.5%, Val avg loss: 3.672769 \n",
      "\n",
      "------------ Epoch 67 ---------------\n",
      "Val accuracy: 19.1%, Val avg loss: 3.700920 \n",
      "\n",
      "------------ Epoch 68 ---------------\n",
      "Val accuracy: 19.5%, Val avg loss: 3.711257 \n",
      "\n",
      "------------ Epoch 69 ---------------\n",
      "Val accuracy: 20.3%, Val avg loss: 3.661981 \n",
      "\n",
      "------------ Epoch 70 ---------------\n",
      "Val accuracy: 18.5%, Val avg loss: 3.760658 \n",
      "\n",
      "------------ Epoch 71 ---------------\n",
      "Val accuracy: 19.8%, Val avg loss: 3.661660 \n",
      "\n",
      "------------ Epoch 72 ---------------\n",
      "Val accuracy: 17.3%, Val avg loss: 3.760793 \n",
      "\n",
      "------------ Epoch 73 ---------------\n",
      "Val accuracy: 18.7%, Val avg loss: 3.726443 \n",
      "\n",
      "------------ Epoch 74 ---------------\n",
      "Val accuracy: 19.4%, Val avg loss: 3.695961 \n",
      "\n",
      "------------ Epoch 75 ---------------\n",
      "Val accuracy: 19.0%, Val avg loss: 3.691006 \n",
      "\n",
      "------------ Epoch 76 ---------------\n",
      "Val accuracy: 19.1%, Val avg loss: 3.683525 \n",
      "\n",
      "------------ Epoch 77 ---------------\n",
      "Val accuracy: 19.6%, Val avg loss: 3.677016 \n",
      "\n",
      "------------ Epoch 78 ---------------\n",
      "Val accuracy: 17.7%, Val avg loss: 3.711810 \n",
      "\n",
      "------------ Epoch 79 ---------------\n",
      "Val accuracy: 18.9%, Val avg loss: 3.733155 \n",
      "\n",
      "------------ Epoch 80 ---------------\n",
      "Val accuracy: 20.0%, Val avg loss: 3.693148 \n",
      "\n",
      "------------ Epoch 81 ---------------\n",
      "Val accuracy: 16.7%, Val avg loss: 3.761967 \n",
      "\n",
      "------------ Epoch 82 ---------------\n",
      "Val accuracy: 20.4%, Val avg loss: 3.660277 \n",
      "\n",
      "------------ Epoch 83 ---------------\n",
      "Val accuracy: 15.3%, Val avg loss: 3.841265 \n",
      "\n",
      "------------ Epoch 84 ---------------\n",
      "Val accuracy: 18.9%, Val avg loss: 3.690693 \n",
      "\n",
      "------------ Epoch 85 ---------------\n",
      "Val accuracy: 20.0%, Val avg loss: 3.650962 \n",
      "\n",
      "------------ Epoch 86 ---------------\n",
      "Val accuracy: 19.9%, Val avg loss: 3.659628 \n",
      "\n",
      "------------ Epoch 87 ---------------\n",
      "Val accuracy: 18.8%, Val avg loss: 3.723493 \n",
      "\n",
      "------------ Epoch 88 ---------------\n",
      "Val accuracy: 19.1%, Val avg loss: 3.696363 \n",
      "\n",
      "------------ Epoch 89 ---------------\n",
      "Val accuracy: 19.3%, Val avg loss: 3.706661 \n",
      "\n",
      "------------ Epoch 90 ---------------\n",
      "Val accuracy: 16.2%, Val avg loss: 3.804232 \n",
      "\n",
      "------------ Epoch 91 ---------------\n",
      "Val accuracy: 19.4%, Val avg loss: 3.659793 \n",
      "\n",
      "------------ Epoch 92 ---------------\n",
      "Val accuracy: 19.4%, Val avg loss: 3.706436 \n",
      "\n",
      "------------ Epoch 93 ---------------\n",
      "Val accuracy: 19.2%, Val avg loss: 3.677836 \n",
      "\n",
      "------------ Epoch 94 ---------------\n",
      "Val accuracy: 20.1%, Val avg loss: 3.651030 \n",
      "\n",
      "------------ Epoch 95 ---------------\n",
      "Val accuracy: 20.5%, Val avg loss: 3.657965 \n",
      "\n",
      "------------ Epoch 96 ---------------\n",
      "Val accuracy: 14.9%, Val avg loss: 3.799348 \n",
      "\n",
      "------------ Epoch 97 ---------------\n",
      "Val accuracy: 20.1%, Val avg loss: 3.658468 \n",
      "\n",
      "------------ Epoch 98 ---------------\n",
      "Val accuracy: 19.2%, Val avg loss: 3.665098 \n",
      "\n",
      "------------ Epoch 99 ---------------\n",
      "Val accuracy: 20.0%, Val avg loss: 3.641812 \n",
      "\n",
      "------------ Epoch 100 ---------------\n",
      "Val accuracy: 18.6%, Val avg loss: 3.682567 \n",
      "\n",
      "------------ Epoch 101 ---------------\n",
      "Val accuracy: 19.5%, Val avg loss: 3.673037 \n",
      "\n",
      "------------ Epoch 102 ---------------\n",
      "Val accuracy: 17.8%, Val avg loss: 3.724581 \n",
      "\n",
      "------------ Epoch 103 ---------------\n",
      "Val accuracy: 18.8%, Val avg loss: 3.716617 \n",
      "\n",
      "------------ Epoch 104 ---------------\n",
      "Val accuracy: 13.0%, Val avg loss: 3.911863 \n",
      "\n",
      "------------ Epoch 105 ---------------\n",
      "Val accuracy: 20.5%, Val avg loss: 3.678045 \n",
      "\n",
      "------------ Epoch 106 ---------------\n",
      "Val accuracy: 21.8%, Val avg loss: 3.618755 \n",
      "\n",
      "------------ Epoch 107 ---------------\n",
      "Val accuracy: 20.4%, Val avg loss: 3.655260 \n",
      "\n",
      "------------ Epoch 108 ---------------\n",
      "Val accuracy: 19.5%, Val avg loss: 3.701418 \n",
      "\n",
      "------------ Epoch 109 ---------------\n",
      "Val accuracy: 19.2%, Val avg loss: 3.720879 \n",
      "\n",
      "------------ Epoch 110 ---------------\n",
      "Val accuracy: 19.1%, Val avg loss: 3.682984 \n",
      "\n",
      "------------ Epoch 111 ---------------\n",
      "Val accuracy: 19.3%, Val avg loss: 3.727716 \n",
      "\n",
      "------------ Epoch 112 ---------------\n",
      "Val accuracy: 15.4%, Val avg loss: 3.806506 \n",
      "\n",
      "------------ Epoch 113 ---------------\n",
      "Val accuracy: 19.8%, Val avg loss: 3.701091 \n",
      "\n",
      "------------ Epoch 114 ---------------\n",
      "Val accuracy: 19.6%, Val avg loss: 3.650065 \n",
      "\n",
      "------------ Epoch 115 ---------------\n",
      "Val accuracy: 20.3%, Val avg loss: 3.644956 \n",
      "\n",
      "------------ Epoch 116 ---------------\n",
      "Val accuracy: 14.2%, Val avg loss: 3.859395 \n",
      "\n",
      "------------ Epoch 117 ---------------\n",
      "Val accuracy: 19.5%, Val avg loss: 3.691432 \n",
      "\n",
      "------------ Epoch 118 ---------------\n",
      "Val accuracy: 17.2%, Val avg loss: 3.694164 \n",
      "\n",
      "------------ Epoch 119 ---------------\n",
      "Val accuracy: 19.7%, Val avg loss: 3.663446 \n",
      "\n",
      "------------ Epoch 120 ---------------\n",
      "Val accuracy: 19.9%, Val avg loss: 3.669225 \n",
      "\n",
      "------------ Epoch 121 ---------------\n",
      "Val accuracy: 20.0%, Val avg loss: 3.636787 \n",
      "\n",
      "------------ Epoch 122 ---------------\n",
      "Val accuracy: 18.9%, Val avg loss: 3.702727 \n",
      "\n",
      "------------ Epoch 123 ---------------\n",
      "Val accuracy: 19.8%, Val avg loss: 3.669662 \n",
      "\n",
      "------------ Epoch 124 ---------------\n",
      "Val accuracy: 18.6%, Val avg loss: 3.654582 \n",
      "\n",
      "------------ Epoch 125 ---------------\n",
      "Val accuracy: 19.4%, Val avg loss: 3.655580 \n",
      "\n",
      "------------ Epoch 126 ---------------\n",
      "Val accuracy: 20.6%, Val avg loss: 3.654558 \n",
      "\n",
      "------------ Epoch 127 ---------------\n",
      "Val accuracy: 20.0%, Val avg loss: 3.674122 \n",
      "\n",
      "------------ Epoch 128 ---------------\n",
      "Val accuracy: 18.0%, Val avg loss: 3.686103 \n",
      "\n",
      "------------ Epoch 129 ---------------\n",
      "Val accuracy: 20.4%, Val avg loss: 3.648766 \n",
      "\n",
      "------------ Epoch 130 ---------------\n",
      "Val accuracy: 20.0%, Val avg loss: 3.668112 \n",
      "\n",
      "------------ Epoch 131 ---------------\n",
      "Val accuracy: 19.2%, Val avg loss: 3.690648 \n",
      "\n",
      "------------ Epoch 132 ---------------\n",
      "Val accuracy: 21.4%, Val avg loss: 3.602643 \n",
      "\n",
      "------------ Epoch 133 ---------------\n",
      "Val accuracy: 19.6%, Val avg loss: 3.673348 \n",
      "\n",
      "------------ Epoch 134 ---------------\n",
      "Val accuracy: 19.2%, Val avg loss: 3.653469 \n",
      "\n",
      "------------ Epoch 135 ---------------\n",
      "Val accuracy: 20.5%, Val avg loss: 3.628252 \n",
      "\n",
      "------------ Epoch 136 ---------------\n",
      "Val accuracy: 18.5%, Val avg loss: 3.712431 \n",
      "\n",
      "------------ Epoch 137 ---------------\n",
      "Val accuracy: 20.8%, Val avg loss: 3.598820 \n",
      "\n",
      "------------ Epoch 138 ---------------\n",
      "Val accuracy: 19.2%, Val avg loss: 3.658147 \n",
      "\n",
      "------------ Epoch 139 ---------------\n",
      "Val accuracy: 18.7%, Val avg loss: 3.675013 \n",
      "\n",
      "------------ Epoch 140 ---------------\n",
      "Val accuracy: 19.4%, Val avg loss: 3.665519 \n",
      "\n",
      "------------ Epoch 141 ---------------\n",
      "Val accuracy: 19.1%, Val avg loss: 3.701098 \n",
      "\n",
      "------------ Epoch 142 ---------------\n",
      "Val accuracy: 19.7%, Val avg loss: 3.668636 \n",
      "\n",
      "------------ Epoch 143 ---------------\n",
      "Val accuracy: 21.2%, Val avg loss: 3.589719 \n",
      "\n",
      "------------ Epoch 144 ---------------\n",
      "Val accuracy: 18.9%, Val avg loss: 3.704035 \n",
      "\n",
      "------------ Epoch 145 ---------------\n",
      "Val accuracy: 19.2%, Val avg loss: 3.698788 \n",
      "\n",
      "------------ Epoch 146 ---------------\n",
      "Val accuracy: 19.6%, Val avg loss: 3.643229 \n",
      "\n",
      "------------ Epoch 147 ---------------\n",
      "Val accuracy: 20.1%, Val avg loss: 3.648882 \n",
      "\n",
      "------------ Epoch 148 ---------------\n",
      "Val accuracy: 18.6%, Val avg loss: 3.685590 \n",
      "\n",
      "------------ Epoch 149 ---------------\n",
      "Val accuracy: 18.5%, Val avg loss: 3.729549 \n",
      "\n",
      "------------ Epoch 150 ---------------\n",
      "Val accuracy: 18.3%, Val avg loss: 3.660704 \n",
      "\n",
      "------------ Epoch 151 ---------------\n",
      "Val accuracy: 20.7%, Val avg loss: 3.599786 \n",
      "\n",
      "------------ Epoch 152 ---------------\n",
      "Val accuracy: 16.3%, Val avg loss: 3.757542 \n",
      "\n",
      "------------ Epoch 153 ---------------\n",
      "Val accuracy: 18.6%, Val avg loss: 3.665214 \n",
      "\n",
      "------------ Epoch 154 ---------------\n",
      "Val accuracy: 18.1%, Val avg loss: 3.709025 \n",
      "\n",
      "------------ Epoch 155 ---------------\n",
      "Val accuracy: 19.6%, Val avg loss: 3.653262 \n",
      "\n",
      "------------ Epoch 156 ---------------\n",
      "Val accuracy: 18.9%, Val avg loss: 3.663764 \n",
      "\n",
      "------------ Epoch 157 ---------------\n",
      "Val accuracy: 20.8%, Val avg loss: 3.627989 \n",
      "\n",
      "------------ Epoch 158 ---------------\n",
      "Val accuracy: 20.0%, Val avg loss: 3.643030 \n",
      "\n",
      "------------ Epoch 159 ---------------\n",
      "Val accuracy: 19.4%, Val avg loss: 3.674167 \n",
      "\n",
      "------------ Epoch 160 ---------------\n",
      "Val accuracy: 19.3%, Val avg loss: 3.632239 \n",
      "\n",
      "------------ Epoch 161 ---------------\n",
      "Val accuracy: 16.4%, Val avg loss: 3.713650 \n",
      "\n",
      "------------ Epoch 162 ---------------\n",
      "Val accuracy: 20.3%, Val avg loss: 3.645313 \n",
      "\n",
      "------------ Epoch 163 ---------------\n",
      "Val accuracy: 14.2%, Val avg loss: 3.872891 \n",
      "\n",
      "------------ Epoch 164 ---------------\n",
      "Val accuracy: 19.4%, Val avg loss: 3.677235 \n",
      "\n",
      "------------ Epoch 165 ---------------\n",
      "Val accuracy: 16.8%, Val avg loss: 3.799990 \n",
      "\n",
      "------------ Epoch 166 ---------------\n",
      "Val accuracy: 20.8%, Val avg loss: 3.614991 \n",
      "\n",
      "------------ Epoch 167 ---------------\n",
      "Val accuracy: 20.1%, Val avg loss: 3.631307 \n",
      "\n",
      "------------ Epoch 168 ---------------\n",
      "Val accuracy: 20.6%, Val avg loss: 3.618361 \n",
      "\n",
      "------------ Epoch 169 ---------------\n",
      "Val accuracy: 21.3%, Val avg loss: 3.608370 \n",
      "\n",
      "------------ Epoch 170 ---------------\n",
      "Val accuracy: 17.8%, Val avg loss: 3.728688 \n",
      "\n",
      "------------ Epoch 171 ---------------\n",
      "Val accuracy: 21.4%, Val avg loss: 3.613080 \n",
      "\n",
      "------------ Epoch 172 ---------------\n",
      "Val accuracy: 20.2%, Val avg loss: 3.663594 \n",
      "\n",
      "------------ Epoch 173 ---------------\n",
      "Val accuracy: 20.8%, Val avg loss: 3.610246 \n",
      "\n",
      "------------ Epoch 174 ---------------\n",
      "Val accuracy: 13.7%, Val avg loss: 3.840864 \n",
      "\n",
      "------------ Epoch 175 ---------------\n",
      "Val accuracy: 17.7%, Val avg loss: 3.794223 \n",
      "\n",
      "------------ Epoch 176 ---------------\n",
      "Val accuracy: 21.9%, Val avg loss: 3.573749 \n",
      "\n",
      "------------ Epoch 177 ---------------\n",
      "Val accuracy: 18.0%, Val avg loss: 3.710851 \n",
      "\n",
      "------------ Epoch 178 ---------------\n",
      "Val accuracy: 19.3%, Val avg loss: 3.660707 \n",
      "\n",
      "------------ Epoch 179 ---------------\n",
      "Val accuracy: 18.7%, Val avg loss: 3.693497 \n",
      "\n",
      "------------ Epoch 180 ---------------\n",
      "Val accuracy: 18.4%, Val avg loss: 3.724939 \n",
      "\n",
      "------------ Epoch 181 ---------------\n",
      "Val accuracy: 20.6%, Val avg loss: 3.582032 \n",
      "\n",
      "------------ Epoch 182 ---------------\n",
      "Val accuracy: 14.6%, Val avg loss: 4.021929 \n",
      "\n",
      "------------ Epoch 183 ---------------\n",
      "Val accuracy: 18.7%, Val avg loss: 3.725533 \n",
      "\n",
      "------------ Epoch 184 ---------------\n",
      "Val accuracy: 19.5%, Val avg loss: 3.632054 \n",
      "\n",
      "------------ Epoch 185 ---------------\n",
      "Val accuracy: 18.1%, Val avg loss: 3.732951 \n",
      "\n",
      "------------ Epoch 186 ---------------\n",
      "Val accuracy: 19.8%, Val avg loss: 3.635117 \n",
      "\n",
      "------------ Epoch 187 ---------------\n",
      "Val accuracy: 20.8%, Val avg loss: 3.597279 \n",
      "\n",
      "------------ Epoch 188 ---------------\n",
      "Val accuracy: 20.5%, Val avg loss: 3.626156 \n",
      "\n",
      "------------ Epoch 189 ---------------\n",
      "Val accuracy: 15.9%, Val avg loss: 3.831592 \n",
      "\n",
      "------------ Epoch 190 ---------------\n",
      "Val accuracy: 20.8%, Val avg loss: 3.622963 \n",
      "\n",
      "------------ Epoch 191 ---------------\n",
      "Val accuracy: 16.7%, Val avg loss: 3.748668 \n",
      "\n",
      "------------ Epoch 192 ---------------\n",
      "Val accuracy: 15.7%, Val avg loss: 4.003669 \n",
      "\n",
      "------------ Epoch 193 ---------------\n",
      "Val accuracy: 20.9%, Val avg loss: 3.593832 \n",
      "\n",
      "------------ Epoch 194 ---------------\n",
      "Val accuracy: 14.4%, Val avg loss: 3.831594 \n",
      "\n",
      "------------ Epoch 195 ---------------\n",
      "Val accuracy: 17.8%, Val avg loss: 3.659039 \n",
      "\n",
      "------------ Epoch 196 ---------------\n",
      "Val accuracy: 16.6%, Val avg loss: 3.735774 \n",
      "\n",
      "------------ Epoch 197 ---------------\n",
      "Val accuracy: 19.7%, Val avg loss: 3.636435 \n",
      "\n",
      "------------ Epoch 198 ---------------\n",
      "Val accuracy: 18.2%, Val avg loss: 3.763690 \n",
      "\n",
      "------------ Epoch 199 ---------------\n",
      "Val accuracy: 20.5%, Val avg loss: 3.651293 \n",
      "\n",
      "------------ Epoch 200 ---------------\n",
      "Val accuracy: 16.1%, Val avg loss: 3.817949 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "for t in range(epochs):\n",
    "    print(f\"------------ Epoch {t+1} ---------------\")\n",
    "    train(train_loader, model, loss_fn, optimizer)\n",
    "    valid(val_loader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
