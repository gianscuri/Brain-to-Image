{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Dataset tutorial: A large and rich EEG dataset for modeling human visual object recognition**\n",
        "\n",
        "In this tutorial you will load, visualize and model the preprocessed EEG data, the corresponding image conditions and the deep neural network (DNN) feature maps of these image conditions from the data paper [A large and rich EEG dataset for modeling human visual object recognition][paper_link]. This dataset contains the EEG responses of 10 participants to 16,540 training image conditions (with 4 repeats per condition) and 200 test image conditions (with 80 repeats per condition). [This video][rsvp] shows you the rapid serial visual presentation (RSVP) paradigm used to collect the EEG dataset. The largness of this dataset enables the modeling of EEG data through several machine learning and deep learning methods, including building end-to-end encoding models of EEG responses to arbitrary images starting from randomly initialized DNNs. The complete dataset (in its raw and preprocessed format) along with the stimuli images, DNN feature maps and detailed data descriptions are available on [OSF][osf]. Please refer to the [paper][paper_link] for information regarding stimuli images, experimental paradigm, EEG acquisition/preprocessing, DNN feature maps extraction and dataset validation through several computational modeling approaches. All the paper results can be reproduces through the code available on [GitHub][git].\n",
        "\n",
        "The data used in this tutorial is found in a Google Drive public folder called [```tutorial_data```][data]. Before running the tutorial code you need to select this folder and choose \"Add a shortcut to Drive\". This will create a shortcut (without copying or taking space) of the folder to a desired path in your Google Drive, from which you can read the content after mounting using ```drive.mount()```.\n",
        "\n",
        "[paper_link]: https://www.biorxiv.org/content/10.1101/2022.03.15.484473v1\n",
        "[rsvp]: https://youtu.be/JhpvpHlfPlE\n",
        "[osf]: https://osf.io/3jk45/\n",
        "[git]: https://github.com/gifale95/eeg_encoding\n",
        "[data]: https://drive.google.com/drive/folders/1ZejP4F9Isj2A_QSlxbPzvOP16-BdBerk?usp=sharing"
      ],
      "metadata": {
        "id": "5W6molWDNK7j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Import libraries and mount Google Drive to access the data\n",
        "\n",
        "To start, you will import the Python libraries used throughout the tutorial and mount your Google Drive to access the data. Make sure that you edit the ```gdrive_data_parent_dir``` variable with the path to the ```tutorial_data``` shortcut folder you added to your Drive.\n",
        "\n"
      ],
      "metadata": {
        "id": "vAFIjHi2QxqT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v1C4LS6PMFu1"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from matplotlib import pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from scipy.stats import pearsonr as corr\n",
        "\n",
        "drive.mount('/content/gdrive/', force_remount=True)\n",
        "gdrive_data_parent_dir = '/content/gdrive/MyDrive/.../tutorial_data' #@param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Load the preprocessed EEG data\n",
        "\n",
        "Now you will load participant's 1 preprocessed EEG data. The EEG data consists of two files:\n",
        "* ```preprocessed_eeg_training.npy```: the preprocessed EEG training data.\n",
        "* ```preprocessed_eeg_test.npy```: the preprocessed EEG test data.\n",
        "\n",
        "Both files are Python dictionaries with the following keys:\n",
        "* ```preprocessed_eeg_data```: preprocessed EEG data in a 4-dimensional array of shape [16,540/200 train/test image conditions × 4/80 train/test EEG repetitions × 17 EEG channels × 100 EEG time points].\n",
        "* ```ch_names```: names of the 17 occipital and parietal EEG channels retained during preprocessing.\n",
        "* ```times```: the 100 time points of each EEG epoch (in seconds, with respect to stimulus onset)."
      ],
      "metadata": {
        "id": "A-ndiaD_Q_Zy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eeg_parent_dir = os.path.join(gdrive_data_parent_dir, 'eeg_dataset',\n",
        "    'preprocessed_data', 'sub-01')\n",
        "eeg_data_train = np.load(os.path.join(eeg_parent_dir,\n",
        "    'preprocessed_eeg_training.npy'), allow_pickle=True).item()\n",
        "eeg_data_test = np.load(os.path.join(eeg_parent_dir,\n",
        "    'preprocessed_eeg_test.npy'), allow_pickle=True).item()\n",
        "\n",
        "print('Training EEG data shape:')\n",
        "print(eeg_data_train['preprocessed_eeg_data'].shape)\n",
        "print('(Training image conditions × Training EEG repetitions × EEG channels × '\n",
        "    'EEG time points)')\n",
        "\n",
        "print('\\nTest EEG data shape:')\n",
        "print(eeg_data_test['preprocessed_eeg_data'].shape)\n",
        "print('(Test image conditions × Test EEG repetitions × EEG channels × '\n",
        "    'EEG time points)')\n",
        "\n",
        "print('\\nEEG channels:')\n",
        "for c,chan in enumerate(eeg_data_train['ch_names']):\n",
        "    print(c, chan)\n",
        "\n",
        "print('\\nEEG time points (in seconds):')\n",
        "for t,time in enumerate(eeg_data_train['times']):\n",
        "    print(t, np.round(time, decimals=2))"
      ],
      "metadata": {
        "id": "qWv4kohCS3V_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Visualize the preprocessed EEG data ERPs\n",
        "\n",
        "Here you will plot the preprocessed EEG data event related potentials (ERPs) across time, for each EEG channel. ERPs consist in the EEG signal averaged across image conditions and repetitions.\n",
        "\n",
        "The ERPs show peaks of activity every 200ms, consistent with the 200ms stimulus onset asynchronies (SOAs) of the rapid serial visual presentation (RSVP) paradigm used to collect the EEG data."
      ],
      "metadata": {
        "id": "QTGny2T-axFh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "erp_data_train = np.mean(eeg_data_train['preprocessed_eeg_data'], 1)\n",
        "erp_data_test = np.mean(eeg_data_test['preprocessed_eeg_data'], 1)\n",
        "erp_data_all = np.mean(np.append(erp_data_train, erp_data_test, 0), 0)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot([-.2, .8], [0, 0], 'k--', [0, 0], [-1.5, 1.5], 'k--')\n",
        "plt.plot(eeg_data_train['times'], np.transpose(erp_data_all));\n",
        "plt.xlabel('Time (s)');\n",
        "plt.xlim(left=-.2, right=.8)\n",
        "plt.ylabel('Voltage');\n",
        "plt.ylim(bottom=-1.5, top=1.5)\n",
        "plt.title('EEG ERPs');"
      ],
      "metadata": {
        "id": "JavyDwOBa-6Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Load the images metadata\n",
        "\n",
        "Next, you will load and visualize the metadata of the EEG responses' stimuli images. The images are divided into a training and a test partition (corresponding to the EEG training and test data partitions). The training partition has 1,654 object concepts, with 10 images per concept, for a total of 16,540 image conditions. The test partition has 200 object concepts with 1 image per concept, for a total of 200 image conditions."
      ],
      "metadata": {
        "id": "8KA8k72sZI91"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img_parent_dir  = os.path.join(gdrive_data_parent_dir, 'image_set')\n",
        "img_metadata = np.load(os.path.join(img_parent_dir, 'image_metadata.npy'),\n",
        "\tallow_pickle=True).item()\n",
        "\n",
        "n_train_img = len(img_metadata['train_img_concepts'])\n",
        "n_train_concepts = len(np.unique(img_metadata['train_img_concepts']))\n",
        "n_train_img_per_concept = int(n_train_img / n_train_concepts)\n",
        "print('Training images: ' + str(n_train_img))\n",
        "print('Image concepts: ' + str(n_train_concepts))\n",
        "print('Images per concept: '+ str(n_train_img_per_concept))\n",
        "\n",
        "n_test_img = len(img_metadata['test_img_concepts'])\n",
        "n_test_concepts = len(np.unique(img_metadata['test_img_concepts']))\n",
        "n_test_img_per_concept = int(n_test_img / n_test_concepts)\n",
        "print('\\nTest images: ' + str(n_test_img))\n",
        "print('Image concepts: ' + str(n_test_concepts))\n",
        "print('Images per concept: '+ str(n_test_img_per_concept))"
      ],
      "metadata": {
        "id": "9BuwHUt5ZtCN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Visualize the training images, and link them to the corresponding EEG responses\n",
        "\n",
        "Here you can visualize arbitrary images from the **training** partition, and link them to their corresponding EEG responses. Select the image of interest by editing the ```train_img_idx``` variable with values in the range [0 16,539] (Python indexing stars from 0)."
      ],
      "metadata": {
        "id": "0cwP6BaFawHB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_img_idx =  0 #@param {type:\"integer\"}\n",
        "\n",
        "eeg_data_single_image = eeg_data_train['preprocessed_eeg_data'][train_img_idx]\n",
        "print('Training EEG single image data shape:')\n",
        "print(eeg_data_single_image.shape)\n",
        "print('(Training EEG repetitions × EEG channels × EEG time points)\\n')\n",
        "\n",
        "train_img_dir = os.path.join(img_parent_dir, 'training_images',\n",
        "\timg_metadata['train_img_concepts'][train_img_idx],\n",
        "\timg_metadata['train_img_files'][train_img_idx])\n",
        "train_img = Image.open(train_img_dir).convert('RGB')\n",
        "\n",
        "plt.figure()\n",
        "plt.axis('off')\n",
        "plt.imshow(train_img)\n",
        "plt.title('Training image: ' + str(train_img_idx+1) + '\\nImage concept: ' +\\\n",
        "\timg_metadata['train_img_concepts'][train_img_idx] + '\\nImage file: ' +\\\n",
        "\timg_metadata['train_img_files'][train_img_idx]);"
      ],
      "metadata": {
        "id": "hvLOYI_nbUif"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Visualize the test images, and link them to the corresponding EEG responses\n",
        "\n",
        "Here you can visualize arbitrary images from the test partition, and link them to their corresponding EEG responses. Select the image of interest by editing the ```test_img_idx``` variable with values in the range [0 199] (Python indexing stars from 0)."
      ],
      "metadata": {
        "id": "mhtPlDiJbtX5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_img_idx =  0 #@param {type:\"integer\"}\n",
        "\n",
        "eeg_data_single_image = eeg_data_test['preprocessed_eeg_data'][train_img_idx]\n",
        "print('Test EEG single image data shape:')\n",
        "print(eeg_data_single_image.shape)\n",
        "print('(Test EEG repetitions × EEG channels × EEG time points)\\n')\n",
        "\n",
        "test_img_dir = os.path.join(img_parent_dir, 'test_images',\n",
        "\timg_metadata['test_img_concepts'][test_img_idx],\n",
        "\timg_metadata['test_img_files'][test_img_idx])\n",
        "test_img = Image.open(test_img_dir).convert('RGB')\n",
        "\n",
        "plt.figure()\n",
        "plt.axis('off')\n",
        "plt.imshow(test_img)\n",
        "plt.title('Test image: ' + str(test_img_idx+1) + '\\nImage concept: ' +\\\n",
        "\timg_metadata['test_img_concepts'][test_img_idx] + '\\nImage file: ' +\\\n",
        "\timg_metadata['test_img_files'][test_img_idx]);"
      ],
      "metadata": {
        "id": "pO1ATgRbb_lU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Load the DNN feature maps\n",
        "\n",
        "Now you will load the training an test images feature maps of a pretrained AlexNet architecture, that have been appended across all DNN layers and downsampled with principal component analysis (PCA). The DNN feature consists of two files:\n",
        "* ```pca_feature_maps_training.npy```: the training DNN feature maps.\n",
        "* ```pca_feature_maps_test.npy```: the test DNN feature maps.\n",
        "\n",
        "Both files are Python dictionaries with the following key:\n",
        "* ```all_layers```: DNN feature maps in a 2-dimensional array of shape [16,540/200 train/test image conditions × 3000 DNN feature maps PCA components]."
      ],
      "metadata": {
        "id": "KDHWAPHmGowk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dnn_parent_dir = os.path.join(gdrive_data_parent_dir, 'dnn_feature_maps',\n",
        "    'pca_feature_maps', 'alexnet', 'pretrained-True', 'layers-all')\n",
        "dnn_fmaps_train = np.load(os.path.join(dnn_parent_dir,\n",
        "    'pca_feature_maps_training.npy'), allow_pickle=True).item()\n",
        "dnn_fmaps_test = np.load(os.path.join(dnn_parent_dir,\n",
        "    'pca_feature_maps_test.npy'), allow_pickle=True).item()\n",
        "\n",
        "print('Training DNN feature maps shape:')\n",
        "print(dnn_fmaps_train['all_layers'].shape)\n",
        "print('(Training image conditions × DNN feature maps PCA components)')\n",
        "\n",
        "print('\\nTraining DNN feature maps shape:')\n",
        "print(dnn_fmaps_test['all_layers'].shape)\n",
        "print('(Test image conditions × DNN feature maps PCA components)')"
      ],
      "metadata": {
        "id": "EMCXdubcGqw1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Train and test linearizing encoding models of EEG visual responses\n",
        "\n",
        "In this last step you will train linearizing encoding models of EEG visual responses, that is, algorithms that predict the EEG responses to arbitrary stimuli images. Linearizing encoding models consist of linear regressions that map the DNN feature maps of certain image conditions onto the corresponding EEG responses. You will train independent linear regressions for each EEG feature (i.e., each combination of EEG channels and time points).\n",
        "\n",
        "You will train linear regressions using the training DNN feature maps and training EEG data (averaged across repetitions), and use the learned regression weights to predict the EEG responses to the test images (using the test DNN feature maps). To reduce computational load, the training/test DNN feature maps are reduced to 100 PCA components.\n",
        "\n",
        "Finally, you will quantify the encoding models prediction accuracy by correlating each predicted EEG feature with the corresponding biological/ground truth EEG feature (averaged across repetitions) across the 200 test image conditions, average the resulting correlation scores over channels, and plot them across time points."
      ],
      "metadata": {
        "id": "BTZpip8w58x3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pred_eeg_data_test = np.zeros((len(eeg_data_test['preprocessed_eeg_data']),\n",
        "    len(eeg_data_test['ch_names']),len(eeg_data_test['times'])))\n",
        "for t in tqdm(range(len(eeg_data_test['times'])), desc='Encoding training'):\n",
        "    for c in range(len(eeg_data_test['ch_names'])):\n",
        "        reg = LinearRegression().fit(dnn_fmaps_train['all_layers'][:,:100],\n",
        "            np.mean(eeg_data_train['preprocessed_eeg_data'][:,:,c,t], 1))\n",
        "        pred_eeg_data_test[:,c,t] = reg.predict(\n",
        "            dnn_fmaps_test['all_layers'][:,:100])\n",
        "\n",
        "encoding_accuracy = np.zeros((len(eeg_data_test['ch_names']),\n",
        "    len(eeg_data_test['times'])))\n",
        "for t in range(len(eeg_data_test['times'])):\n",
        "    for c in range(len(eeg_data_test['ch_names'])):\n",
        "\t    encoding_accuracy[c,t] = corr(pred_eeg_data_test[:,c,t],\n",
        "            np.mean(eeg_data_test['preprocessed_eeg_data'][:,:,c,t], 1))[0]\n",
        "\n",
        "plt.figure()\n",
        "plt.plot([-.2, .8], [0, 0], 'k--', [0, 0], [-1, 1], 'k--')\n",
        "plt.plot(eeg_data_test['times'], np.mean(encoding_accuracy, 0));\n",
        "plt.xlabel('Time (s)');\n",
        "plt.xlim(left=-.2, right=.8)\n",
        "plt.ylabel('Pearson\\'s $r$');\n",
        "plt.ylim(bottom=-.05, top=.7)\n",
        "plt.title('Encoding accuracy');"
      ],
      "metadata": {
        "id": "ovnFuCje6Aso"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}